{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting Radiation Surfrad .dat Files to NetCDF4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the examples will be data from `Bondville_IL/`, and from the year 2016. Such files will be names `bon16###` where `###` denotes the julian day of the year. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Radiation's Surfrad files have numerous headers:\n",
    "\n",
    "    YEAR DDD MM DD HH mm hh.mmm ZNAGL dw_psp qc uw_psp qc direct qc diffuse qc dw_pir qc dwCasTmp qc dwDomTmp qc uw_pir qc uwCastmp qc uwDomtmp qc uvb qc par qc netSolar qc netIr qc totalNet qc temp qc rh qc windSp qc winsDir qc Baro qc\n",
    "    \n",
    "All of those items, seperated by whitespace, are headers. An initial observation would show that the `qc` header shows up multiple times -- these are quality control flags for the preceding header. A `qc` value of 0 indicated values within an expected range, a value of 1 indicates a value outside of a physically possible range, a value of 2 indicates a value that is physically possible but \"should be used with scrutiny\". Missing values are indicated by a value of \"-9999.9\" and should always have a corresponding `qc` of \"1\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting to .csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The .dat files are plain-text files delimited by whitespace. These files have no headers, only minimal site info and raw data. Here are the first several rows of the file `bon16001.dat`:\n",
    "\n",
    "     Bondville\n",
    "       40.05  -88.37  213 m version 1\n",
    "     2016   1  1  1  0  0  0.000 105.13    -3.4 0     0.0 0     0.8 0    -0.1 0   275.8 0   272.5 0   272.2 0   305.1 0   271.0 0   271.0 0     0.0 0     0.2 0     0.0 0   -29.3 0   -29.3 0    -2.2 0    76.5 0     6.8 0   270.0 0  1001.5 0\n",
    "     2016   1  1  1  0  1  0.017 105.32    -3.6 0     0.0 0     0.8 0    -0.1 0   267.7 0   272.5 0   272.1 0   304.4 0   271.0 0   271.0 0     0.0 0     0.2 0     0.0 0   -36.7 0   -36.7 0    -2.2 0    76.1 0     6.4 0   268.2 0  1001.5 0\n",
    "     2016   1  1  1  0  2  0.033 105.50    -3.6 0     0.0 0     0.7 0    -0.2 0   260.6 0   272.5 0   272.1 0   303.8 0   271.0 0   271.0 0     0.0 0     0.2 0     0.0 0   -43.1 0   -43.1 0    -2.2 0    77.3 0     6.0 0   272.1 0  1001.5 0\n",
    "     2016   1  1  1  0  3  0.050 105.68    -3.7 0     0.0 0     0.4 0    -0.1 0   252.5 0   272.5 0   272.1 0   303.1 0   270.9 0   270.9 0     0.0 0     0.2 0     0.0 0   -50.7 0   -50.7 0    -2.2 0    76.7 0     6.6 0   273.9 0  1001.5 0\n",
    "     2016   1  1  1  0  4  0.067 105.86    -3.8 0     0.0 0     0.4 0    -0.1 0   246.8 0   272.5 0   272.0 0   302.4 0   270.9 0   270.9 0     0.0 0     0.2 0     0.0 0   -55.6 0   -55.6 0    -2.2 0    77.0 0     6.0 0   278.4 0  1001.5 0\n",
    "\n",
    "Converting these files to .csv will essentially double the amount of storage needed for all the data. However, it seems to be a necessary step, due to the nature of the .dat files. The first step is to include the headers in the new .csv file. I've made a file called `headers.txt` (and a matching `headers.dat`) which contains all the headers that match up with Surfrad .dat files (and are delimited by whitespace).\n",
    "\n",
    "What follows can be found in the file `dat_to_csv.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from os.path import basename\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YEAR,DDD,MM,DD,HH,mm,hh.mmm,ZNAGL,dw_psp,qc,uw_psp,qc,direct,qc,diffuse,qc,dw_pir,qc,dwCasTmp,qc,dwDomTmp,qc,uw_pir,qc,uwCastmp,qc,uwDomtmp,qc,uvb,qc,par,qc,netSolar,qc,netIr,qc,totalNet,qc,temp,qc,rh,qc,windSp,qc,winsDir,qc,Baro,qc\n"
     ]
    }
   ],
   "source": [
    "with open(\"headers.dat\", 'r') as header_file:\n",
    "    for line in header_file:\n",
    "        headers = line.split() # this is an array of all the headers\n",
    "headers = ('%s') % ','.join(headers) # this formats the headers as a comma-delimited string\n",
    "\n",
    "print headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input = \"bon16001.dat\" # this is the file for this example, normally passed in via command line\n",
    "base = os.path.splitext(basename(input))[0] # this gets the name of the file (e.g \"bon16001.dat\" -> \"bon16001\")\n",
    "out_name = (base + \".csv\") # this is the name for the file to be written\n",
    "\n",
    "with open(input, 'r') as input_file:\n",
    "    with open(out_name, 'w') as output_file:\n",
    "        output_file.write(headers + \"\\n\") # this writes the headers to the new file\n",
    "        for count, line in enumerate(input_file):\n",
    "            if count < 2: # we can skip the first two lines of the input\n",
    "                pass\n",
    "            else:\n",
    "                if line.split()[0]=='\\x1a': # skip empty rows\n",
    "                    pass\n",
    "                else: # write the data delimited by commas\n",
    "                    outLine = \",\".join(line.split())\n",
    "                    output_file.write(outLine + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I made a shell script which will find all the .dat files in your directory and convert them into .csv files in the `Data/csv` directory. You will need to have the `dat_to_csv.py` file in your directory along with `_dat_to_csv.sh`. You can then run\n",
    "\n",
    "    ./_dat_to_csv.sh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting to .nc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part One - Dataframes for Daily Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even after all the trouble of converting to .csv files, we still can't convert directly to NetCDF files. The data must again be converted, this time into Pandas dataframes. However, dataframes are stored in memory, not in disk, so these are not extra files to be storing.\n",
    "\n",
    "This method is for one-to-one conversions; each input file will get its own output file. The method to create combined files will be in Part Two - Dataframes for Annual Files.\n",
    "\n",
    "What follows can be found in `csv_to_nc_daily.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from os.path import basename\n",
    "import os\n",
    "import pandas as pd\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_testsite(input):\n",
    "    '''\n",
    "    Grabs the initials from the filename and returns the full name of the test site\n",
    "    '''\n",
    "    base = os.path.splitext(basename(input))[0]\n",
    "    testsite = base[0:3]\n",
    "    if testsite == \"bon\":\n",
    "        return \"Bondville_IL\"\n",
    "    elif testsite == \"tbl\":\n",
    "        return \"Boulder_CO\"\n",
    "    elif testsite == \"dra\":\n",
    "        return \"Desert_Rock_NV\"\n",
    "    elif testsite == \"fpk\":\n",
    "        return \"Fort_Peck_MT\"\n",
    "    elif testsite == \"gwn\":\n",
    "        return \"Goodwin_Creek_MS\"\n",
    "    elif testsite == \"psu\":\n",
    "        return \"Penn_State_PA\"\n",
    "    elif testsite == \"sxf\":\n",
    "        return \"Sioux_Falls_SD\"\n",
    "    else:\n",
    "        print \"no matching testing site for input file\"\n",
    "        return \"no_test_site\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input = \"bon16001.csv\"\n",
    "filename = os.path.splitext(basename(input))[0] # same as before: \"bon16001\"\n",
    "\n",
    "with open(input, 'r') as input_file:\n",
    "    df1=pd.read_csv(input_file,\n",
    "        sep=\",\", # comma-delimited\n",
    "        parse_dates = {'Date': [0,1,4,5]},\n",
    "        date_parser = lambda x: pd.to_datetime(x, format=\"%Y %j %H %M\"), # using Year, Julian Day, Hour, and Minute \n",
    "        index_col = ['Date']) # sets the index to the new \"Date\" column\n",
    "    df1.loc[:,'TestSite'] = get_testsite(input) # adds new column with test site (e.g \"Bondville_IL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If you decide you don't need all the qc columns, here's a handy function to get rid of them\n",
    "def filter_qc(df1):\n",
    "    '''\n",
    "    Drops all the qc columns\n",
    "    '''\n",
    "    df1.drop(list(df1.filter(like=\"qc\")), axis=1, inplace=True)\n",
    "    df1.drop(df1.columns[[0,1,2]], axis=1, inplace=True)\n",
    "    return df1\n",
    "\n",
    "# Since we know that \"-9999.9\" indicates a \"NaN\" value, we can replace them like this\n",
    "def replace_nan(df1):\n",
    "    '''\n",
    "    The value \"-9999.9\" is specified to be a placeholder for non-existent values. This replaces those values with \"NaN\"\n",
    "    '''\n",
    "    df1.replace(to_replace=\"-9999.9\",value=\"NaN\", inplace=True)\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To actually write out to a NetCDF file, use xarray\n",
    "xds = xr.Dataset.from_dataframe(df1)\n",
    "xds.to_netcdf(filename + \".nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part Two - Dataframes for Annual Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything above makes sense, everything below should as well. The only difference is that instead of writing out files one-to-one, all the files are combined by year and testsite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from os.path import basename\n",
    "import os\n",
    "import pandas as pd\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are some functions to help\n",
    "def get_out_name(input):\n",
    "    '''\n",
    "    Returns testsite_year\n",
    "    For example, Bondville_IL_16\n",
    "    '''\n",
    "    return \"%s_%s\" % (get_testsite(input),get_year(input))\n",
    "\n",
    "def get_year(input):\n",
    "    '''\n",
    "    Returns the year of the file\n",
    "    For example, bon16001.csv returns 16\n",
    "    '''\n",
    "    return get_filename(input)[3:5]\n",
    "\n",
    "def get_julian_day(input):\n",
    "    '''\n",
    "    Returns the julian day\n",
    "    For example, bon16001.csv returns 001\n",
    "    '''\n",
    "    return get_filename(input)[5:8]\n",
    "\n",
    "def get_filename(input):\n",
    "    '''\n",
    "    Returns the name of the file without the extension.\n",
    "    For example:\n",
    "\n",
    "    Input: \"Test_01.csv\"\n",
    "    Output: \"Test_01\"\n",
    "    '''\n",
    "    return os.path.splitext(basename(input))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input1 = \"bon16001.csv\"\n",
    "input2 = \"bon16002.csv\"\n",
    "filesToProcess = [input1,input2] # make an array of all the files to combine\n",
    "outName = get_out_name(filesToProcess[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are necessary functions for conversion and are not very different from what is shown above\n",
    "\n",
    "def csv_to_dataframe(input):\n",
    "    '''\n",
    "    Loads a csv into a Pandas dataframe.\n",
    "    Uses the Year, Julian Day, Hour, and Minute to create an index column of \"Date\".\n",
    "    Adds the test site as a column\n",
    "    '''\n",
    "    with open(input, 'r') as input_file:\n",
    "        df1=pd.read_csv(input_file,\n",
    "            sep=\",\",\n",
    "            parse_dates = {'Date': [0,1,4,5]},\n",
    "            date_parser = lambda x: pd.to_datetime(x, format=\"%Y %j %H %M\"),\n",
    "            index_col = ['Date'])\n",
    "        df1.loc[:,'TestSite'] = get_testsite(input)\n",
    "    return df1\n",
    "\n",
    "def write_netcdf(df1, name):\n",
    "    '''\n",
    "    Writes the dataframe into a NetCDF4 file\n",
    "    '''\n",
    "    xds = xr.Dataset.from_dataframe(df1)\n",
    "    xds.to_netcdf(name + \".nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for count,input in enumerate(filesToProcess):\n",
    "    filename = get_filename(input)\n",
    "    df1 = csv_to_dataframe(input)\n",
    "    if count == 0: # move the first dataframe into df2, df2 will be a sort of \"master\" dataframe which we append to\n",
    "        df2 = df1\n",
    "        del df1\n",
    "    else:\n",
    "        df2 = pd.concat([df2, df1]) # combines the current dataframe with the \"master\" dataframe\n",
    "        del df1\n",
    "write_netcdf(df2, outName)\n",
    "\n",
    "del df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifically for annual files, an example would be\n",
    "\n",
    "    python csv_to_nc_combine.py bon16*.csv\n",
    "    \n",
    "which would take all the Bondville_IL files from 2016 and combine them into a NetCDF file. You would need to point it to the correct directory which contains you data files. In my case that would be\n",
    "\n",
    "    python csv_to_nc_combine.py Data/csv/bon16*.csv\n",
    "    \n",
    "There is nothing to stop you from trying\n",
    "\n",
    "    python csv_to_nc_combine.py Data/csv/bon*.csv\n",
    "\n",
    "which would combine all the Bondville_IL files. This will take a very long time to run."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
